# Human Evaluation Tool for NLE

## This repo is for our TrustNLP 2024 paper "Tell Me Why: Explainable Public Health Fact-Checking with Large Language Models".

This tool is specifically crafted for human evaluation of Natural Language Explanations. For further details and a comprehensive understanding of all criteria and clear guidelines regarding them, please refer to Section 5.2 (Human Evaluation) of the paper.

## Run the Tool

To run the tool, follow these steps:

```
git clone [repo URL]

cd human-evaluation-tool-for-NLE

pip install requirements.txt

cd human_evaluation_tool

python manage.py runserver
```
